{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNolQYb4QvEjnjFzUzDSpoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelconsigli/Tesi/blob/main/Speech2RIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anton-jeran/Speech2RIR\n",
        "%cd Speech2RIR/\n",
        "!bash download_model.sh\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msOSPJzwbqgm",
        "outputId": "ee3a668b-df01-409a-feaf-c6d3487621d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Speech2RIR'...\n",
            "remote: Enumerating objects: 237, done.\u001b[K\n",
            "remote: Counting objects: 100% (237/237), done.\u001b[K\n",
            "remote: Compressing objects: 100% (228/228), done.\u001b[K\n",
            "remote: Total 237 (delta 55), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (237/237), 214.23 KiB | 6.69 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/Speech2RIR/Speech2RIR\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CcF1c9i76-MVPJ-PGoBwvaVtOUBPD57y\n",
            "From (redirected): https://drive.google.com/uc?id=1CcF1c9i76-MVPJ-PGoBwvaVtOUBPD57y&confirm=t&uuid=15ac1d55-a026-4c0f-bc8e-a3b710c4eb04\n",
            "To: /content/Speech2RIR/Speech2RIR/checkpoint-1040000steps.pkl\n",
            "100% 3.38G/3.38G [01:07<00:00, 49.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TrnXjR-vrCrub_RY6kdEBqA0D4hGiIp3\n",
            "From (redirected): https://drive.google.com/uc?id=1TrnXjR-vrCrub_RY6kdEBqA0D4hGiIp3&confirm=t&uuid=c1d1cd39-a521-4557-b85f-1dc8b220174f\n",
            "To: /content/Speech2RIR/Speech2RIR/checkpoint-1900000steps.pkl\n",
            "100% 3.38G/3.38G [01:13<00:00, 46.1MB/s]\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CARICO IL MODELLO:\n",
        "\n",
        "- Importa le librerie — carica il codice di PyTorch e della rete (modello) specifico che userai.\n",
        "\n",
        "- Istanzia il modello — crea una nuova rete (modello) usando il codice della classe Generator. Qui devi specificare le stesse dimensioni e parametri con cui la rete è stata addestrata, altrimenti i pesi non ci entreranno bene.\n",
        "\n",
        "- Carica i pesi del modello — apre il checkpoint (file con dati salvati) e legge i pesi nella parte giusta del file (checkpoint['model']['generator']).\n",
        "\n",
        "- Inserisci i pesi nel modello — qui è il punto critico: se le dimensioni del modello e dei pesi non combaciano esce errore.\n",
        "\n",
        "- Metti il modello in modalità valutazione — prepara il modello per fare previsioni (usare il modello, non addestrarlo).\n",
        "\n",
        "- Stampa messaggio di conferma."
      ],
      "metadata": {
        "id": "QbiN2-Iy1AsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I pesi verranno utilizzati nella funzione forward del generator, in particolare nell'encoder e nel decoder. Questi due usano pesi interni es.\n",
        "\n",
        "*conv = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3*)\n",
        "\n",
        "- Prima di caricare pesi, i valori dei filtri (kernel) sono casuali.\n",
        "\n",
        "- Dopo conv.load_state_dict(...) con pesi addestrati, quei valori cambiano e diventano precisi per rilevare certe caratteristiche nell'input.\n",
        "\n",
        "- Quando passi un input x a conv(x), il risultato riflette le trasformazioni con quei pesi."
      ],
      "metadata": {
        "id": "pxorWRtd5aSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Import delle librerie\n",
        "import torch\n",
        "from models.autoencoder.AudioDec import Generator\n",
        "\n",
        "# 2️⃣ Crea il modello (con i parametri di default)\n",
        "# Parametri dal file di configurazione YAML\n",
        "model = Generator(\n",
        "    input_channels=1,\n",
        "    output_channels_rir=1,\n",
        "    encode_channels=16,\n",
        "    decode_channels=16,\n",
        "    code_dim=128,\n",
        "    codebook_num=64,\n",
        "    codebook_size=8192,\n",
        "    bias=True,\n",
        "    combine_enc_ratios=[],\n",
        "    combine_enc_strides=[],\n",
        "    seperate_enc_ratios_rir=[2, 4, 8, 12, 16, 32],\n",
        "    seperate_enc_strides_rir=[2, 2, 3, 5, 5, 5],\n",
        "    rir_dec_ratios=[256, 128, 64, 32, 32, 32, 16],\n",
        "    rir_dec_strides=[5, 5, 2, 2, 2, 1, 1],\n",
        "    mode='causal',\n",
        "    codec='audiodec',\n",
        "    projector='conv1d',\n",
        "    quantier='residual_vq',\n",
        ")\n",
        "\n",
        "# 3️⃣ Carica i pesi pre-addestrati\n",
        "checkpoint_path = \"exp/autoencoder/symAD_vctk_48000_hop300/checkpoint-1900000steps.pkl\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "# Carica nello stato del modello solo la parte 'generator' dentro 'model'\n",
        "model.load_state_dict(checkpoint['model']['generator'])\n",
        "\n",
        "# 4️⃣ Metti il modello in modalità \"valutazione\"\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Modello caricato correttamente!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECJNCrMFlS1j",
        "outputId": "8fe1946a-1c72-4781-b8f5-f32f5249cf03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modello caricato correttamente!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- carico e preparo l'audio\n",
        "- stimo la RIR\n",
        "- salvo"
      ],
      "metadata": {
        "id": "0vy9D0gAKEDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 📘 CONFRONTO TRA RIR STIMATA (dal modello) E RIR REALE\n",
        "# ============================================================\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.functional import pearson_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Aggiunge il percorso della directory 'losses' al sys.path\n",
        "sys.path.append('/content/Speech2RIR/Speech2RIR/losses')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1️⃣ CARICAMENTO AUDIO DI INGRESSO (speech riverberata)\n",
        "# ------------------------------------------------------------\n",
        "# L'audio di partenza è il segnale riverberato da cui il modello stima la RIR\n",
        "\n",
        "audio_path = \"/content/Tests/Test 1/impulseresponseheslingtonchurch-006_sing.mp3\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "# 🔸 Porta tutto a 48 kHz (il modello è addestrato così)\n",
        "if sr != 48000:\n",
        "    waveform = torchaudio.functional.resample(waveform, sr, 48000)\n",
        "    sr = 48000\n",
        "\n",
        "# 🔸 Converte in mono (serve 1 solo canale)\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "# 🔸 Aggiunge dimensione batch (B, C, T)\n",
        "waveform = waveform.unsqueeze(0)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2️⃣ STIMA DELLA RIR TRAMITE MODELLO (già caricato come “model”)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    estimated_rir = model(waveform)  # output: (B, 1, T)\n",
        "\n",
        "print(\"✅ RIR stimata dal modello.\")\n",
        "print(\"Shape stimata:\", estimated_rir.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3️⃣ CARICAMENTO DELLA RIR REALE ASSOCIATA\n",
        "# ------------------------------------------------------------\n",
        "rir_real_path = \"/content/Tests/Test 1/impulseresponseheslingtonchurch-006.wav\"\n",
        "rir_real, sr_rir = torchaudio.load(rir_real_path)\n",
        "\n",
        "if sr_rir != 48000:\n",
        "    rir_real = torchaudio.functional.resample(rir_real, sr_rir, 48000)\n",
        "\n",
        "rir_real = rir_real.unsqueeze(0) if rir_real.ndim == 1 else rir_real\n",
        "rir_real = torch.mean(rir_real, dim=0, keepdim=True)  # mono\n",
        "\n",
        "print(\"✅ RIR reale caricata.\")\n",
        "print(\"Shape reale:\", rir_real.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4️⃣ NORMALIZZAZIONE AMPIEZZA (per confronto forma, non livello)\n",
        "# ------------------------------------------------------------\n",
        "estimated_rir = estimated_rir / torch.max(torch.abs(estimated_rir))\n",
        "rir_real = rir_real / torch.max(torch.abs(rir_real))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5️⃣ ALLINEAMENTO TEMPORALE (cross-correlation)\n",
        "# ------------------------------------------------------------\n",
        "# Il modello può introdurre un piccolo ritardo → lo stimiamo e correggiamo\n",
        "corr = F.conv1d(rir_real, torch.flip(estimated_rir, dims=[-1]), padding=rir_real.shape[-1]//2)\n",
        "shift = torch.argmax(corr)\n",
        "shift_amount = int(shift - rir_real.shape[-1]//2)\n",
        "rir_real_aligned = torch.roll(rir_real, -shift_amount, dims=-1)\n",
        "\n",
        "print(f\"Allineamento temporale corretto: shift di {shift_amount} campioni.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6️⃣ TAGLIO A LUNGHEZZA MINIMA COMUNE\n",
        "# ------------------------------------------------------------\n",
        "min_len = min(rir_real_aligned.shape[-1], estimated_rir.shape[-1])\n",
        "rir_real_aligned = rir_real_aligned[..., :min_len]\n",
        "estimated_rir = estimated_rir[..., :min_len]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7️⃣ CALCOLO DELLE METRICHE DI CONFRONTO\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# 🔸 MSE (Mean Squared Error)\n",
        "mse = torch.mean((rir_real_aligned - estimated_rir) ** 2).item()\n",
        "\n",
        "# 🔸 Correlazione di Pearson (PCC)\n",
        "corr_coeff = pearson_corrcoef(rir_real_aligned.flatten(), estimated_rir.flatten()).item()\n",
        "\n",
        "# 🔸 Differenza Spettrale (STFT distance)\n",
        "spec_real = torch.stft(rir_real_aligned.squeeze(), n_fft=1024, hop_length=256, return_complex=True)\n",
        "spec_est = torch.stft(estimated_rir.squeeze(), n_fft=1024, hop_length=256, return_complex=True)\n",
        "stft_distance = torch.mean(torch.abs(torch.log1p(torch.abs(spec_real)) - torch.log1p(torch.abs(spec_est)))).item()\n",
        "print(\"\\n📊 RISULTATI DEL CONFRONTO\")\n",
        "print(f\"• MSE: {mse:.6f}\")\n",
        "print(f\"• Correlazione (PCC): {corr_coeff:.4f}\")\n",
        "print(f\"• Distanza spettrale (STFT): {stft_distance:.6f}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8️⃣ VISUALIZZAZIONE GRAFICA DELLE DUE RIR\n",
        "# ------------------------------------------------------------\n",
        "#time = np.linspace(0, min_len / 48000, min_len)\n",
        "\n",
        "#plt.figure(figsize=(12, 5))\n",
        "##plt.plot(time, rir_real_aligned.squeeze().cpu().numpy(), label='RIR Reale', alpha=0.8)\n",
        "#plt.plot(time, estimated_rir.squeeze().cpu().numpy(), label='RIR Stimata', alpha=0.7)\n",
        "#plt.xlabel(\"Tempo [s]\")\n",
        "#plt.ylabel(\"Ampiezza normalizzata\")\n",
        "#plt.title(\"Confronto tra RIR Reale e RIR Stimata\")\n",
        "#plt.legend()\n",
        "#plt.grid()\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "ditPHrOPKPOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02aa3dd-5d19-4577-b177-bda1704e3d59"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RIR stimata dal modello.\n",
            "Shape stimata: torch.Size([1, 1, 79200])\n",
            "✅ RIR reale caricata.\n",
            "Shape reale: torch.Size([1, 144000])\n",
            "Allineamento temporale corretto: shift di -58689 campioni.\n",
            "\n",
            "📊 RISULTATI DEL CONFRONTO\n",
            "• MSE: 0.004858\n",
            "• Correlazione (PCC): -0.0000\n",
            "• Distanza spettrale (STFT): 0.442082\n"
          ]
        }
      ]
    }
  ]
}