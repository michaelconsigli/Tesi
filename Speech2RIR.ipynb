{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb1ZblcphHFt+1hhJ4j6XJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelconsigli/Tesi/blob/main/Speech2RIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anton-jeran/Speech2RIR\n",
        "%cd Speech2RIR/\n",
        "!bash download_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msOSPJzwbqgm",
        "outputId": "b2b51a91-1b91-43ab-80f5-3a904ec49203"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Speech2RIR'...\n",
            "remote: Enumerating objects: 237, done.\u001b[K\n",
            "remote: Counting objects: 100% (237/237), done.\u001b[K\n",
            "remote: Compressing objects: 100% (228/228), done.\u001b[K\n",
            "remote: Total 237 (delta 55), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (237/237), 214.23 KiB | 7.65 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/Speech2RIR/Speech2RIR\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CcF1c9i76-MVPJ-PGoBwvaVtOUBPD57y\n",
            "From (redirected): https://drive.google.com/uc?id=1CcF1c9i76-MVPJ-PGoBwvaVtOUBPD57y&confirm=t&uuid=cf4f5262-0e27-4cda-8604-6666afb1cbff\n",
            "To: /content/Speech2RIR/Speech2RIR/checkpoint-1040000steps.pkl\n",
            "100% 3.38G/3.38G [00:46<00:00, 72.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TrnXjR-vrCrub_RY6kdEBqA0D4hGiIp3\n",
            "From (redirected): https://drive.google.com/uc?id=1TrnXjR-vrCrub_RY6kdEBqA0D4hGiIp3&confirm=t&uuid=4aa381d3-a805-411f-bf96-b27e4f98fe14\n",
            "To: /content/Speech2RIR/Speech2RIR/checkpoint-1900000steps.pkl\n",
            "100% 3.38G/3.38G [01:07<00:00, 49.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CARICO IL MODELLO:\n",
        "\n",
        "- Importa le librerie — carica il codice di PyTorch e della rete (modello) specifico che userai.\n",
        "\n",
        "- Istanzia il modello — crea una nuova rete (modello) usando il codice della classe Generator. Qui devi specificare le stesse dimensioni e parametri con cui la rete è stata addestrata, altrimenti i pesi non ci entreranno bene.\n",
        "\n",
        "- Carica i pesi del modello — apre il checkpoint (file con dati salvati) e legge i pesi nella parte giusta del file (checkpoint['model']['generator']).\n",
        "\n",
        "- Inserisci i pesi nel modello — qui è il punto critico: se le dimensioni del modello e dei pesi non combaciano esce errore.\n",
        "\n",
        "- Metti il modello in modalità valutazione — prepara il modello per fare previsioni (usare il modello, non addestrarlo).\n",
        "\n",
        "- Stampa messaggio di conferma."
      ],
      "metadata": {
        "id": "QbiN2-Iy1AsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I pesi verranno utilizzati nella funzione forward del generator, in particolare nell'encoder e nel decoder. Questi due usano pesi interni es.\n",
        "\n",
        "*conv = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3*)\n",
        "\n",
        "- Prima di caricare pesi, i valori dei filtri (kernel) sono casuali.\n",
        "\n",
        "- Dopo conv.load_state_dict(...) con pesi addestrati, quei valori cambiano e diventano precisi per rilevare certe caratteristiche nell'input.\n",
        "\n",
        "- Quando passi un input x a conv(x), il risultato riflette le trasformazioni con quei pesi."
      ],
      "metadata": {
        "id": "pxorWRtd5aSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Import delle librerie\n",
        "import torch\n",
        "from models.autoencoder.AudioDec import Generator\n",
        "\n",
        "# 2️⃣ Crea il modello (con i parametri di default)\n",
        "# Parametri dal file di configurazione YAML\n",
        "model = Generator(\n",
        "    input_channels=1,\n",
        "    output_channels_rir=1,\n",
        "    encode_channels=16,\n",
        "    decode_channels=16,\n",
        "    code_dim=128,\n",
        "    codebook_num=64,\n",
        "    codebook_size=8192,\n",
        "    bias=True,\n",
        "    combine_enc_ratios=[],\n",
        "    combine_enc_strides=[],\n",
        "    seperate_enc_ratios_rir=[2, 4, 8, 12, 16, 32],\n",
        "    seperate_enc_strides_rir=[2, 2, 3, 5, 5, 5],\n",
        "    rir_dec_ratios=[256, 128, 64, 32, 32, 32, 16],\n",
        "    rir_dec_strides=[5, 5, 2, 2, 2, 1, 1],\n",
        "    mode='causal',\n",
        "    codec='audiodec',\n",
        "    projector='conv1d',\n",
        "    quantier='residual_vq',\n",
        ")\n",
        "\n",
        "# 3️⃣ Carica i pesi pre-addestrati\n",
        "checkpoint_path = \"exp/autoencoder/symAD_vctk_48000_hop300/checkpoint-1900000steps.pkl\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "# Carica nello stato del modello solo la parte 'generator' dentro 'model'\n",
        "model.load_state_dict(checkpoint['model']['generator'])\n",
        "\n",
        "# 4️⃣ Metti il modello in modalità \"valutazione\"\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Modello caricato correttamente!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECJNCrMFlS1j",
        "outputId": "b1d3dfab-72ef-48a9-b800-ad594c3c3f49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modello caricato correttamente!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- carico e preparo l'audio\n",
        "- stimo la RIR\n",
        "- salvo"
      ],
      "metadata": {
        "id": "0vy9D0gAKEDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "# 1️⃣ Carica l’audio\n",
        "audio_path = \"/content/Tests/example 1/Reverb/impulseresponseheslingtonchurch-006_sing.mp3\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "# 2️⃣ Porta il sample rate a 48000 Hz se serve\n",
        "if sr != 48000:\n",
        "    waveform = torchaudio.functional.resample(waveform, sr, 48000)\n",
        "    sr = 48000\n",
        "\n",
        "# 3️⃣ Converte in mono (il modello accetta 1 canale)\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "# 4️⃣ Aggiungi dimensione batch (B, C, T)\n",
        "waveform = waveform.unsqueeze(0)\n",
        "\n",
        "# 5️⃣ Stima la RIR\n",
        "with torch.no_grad():\n",
        "    estimated_rir = model(waveform)\n",
        "\n",
        "# 6️⃣ Risultato\n",
        "print(\"✅ RIR stimata!\")\n",
        "print(\"Shape:\", estimated_rir.shape)\n"
      ],
      "metadata": {
        "id": "ditPHrOPKPOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8bcf08-d1af-4e95-c47e-32f4a99fc3f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RIR stimata!\n",
            "Shape: torch.Size([1, 1, 79200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faccio il confronto con una RIR reale"
      ],
      "metadata": {
        "id": "XkQo-4ALbKp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica la RIR reale da file (sostituisci con il percorso corretto)\n",
        "rir_reale_path = \"/content/Tests/example 1/RIR_reale/impulseresponseheslingtonchurch-006.wav\"\n",
        "rir_reale, sr_rir = torchaudio.load(rir_reale_path)\n",
        "\n",
        "# Assicurati sample rate 48000 Hz\n",
        "if sr_rir != 48000:\n",
        "    rir_reale = torchaudio.functional.resample(rir_reale, sr_rir, 48000)\n",
        "    sr_rir = 48000\n",
        "\n",
        "# Converti in mono se necessario\n",
        "if rir_reale.shape[0] > 1:\n",
        "    rir_reale = torch.mean(rir_reale, dim=0, keepdim=True)\n",
        "\n",
        "# Aggiungi dimensione batch (B, C, T)\n",
        "rir_reale = rir_reale.unsqueeze(0)\n",
        "\n",
        "# Assicurati che la dimensione temporale (lunghezza) corrisponda esattamente a quella stimata\n",
        "min_len = min(rir_reale.shape[-1], estimated_rir.shape[-1])\n",
        "rir_reale = rir_reale[:, :, :min_len]\n",
        "estimated_rir = estimated_rir[:, :, :min_len]\n",
        "\n",
        "# Calcola errore L2 (mean squared error)\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "errore = mse_loss(estimated_rir, rir_reale)\n",
        "\n",
        "print(\"Errore MSE tra RIR stimata e reale:\", errore.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0cQ7xCFbN2Z",
        "outputId": "2e8bc3b5-8078-4536-e6a1-4716f8c6729d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errore MSE tra RIR stimata e reale: 0.005334637127816677\n"
          ]
        }
      ]
    }
  ]
}