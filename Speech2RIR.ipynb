{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuC+Vj81dGasqYwQuj7hn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelconsigli/Tesi/blob/main/Speech2RIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anton-jeran/Speech2RIR\n",
        "%cd Speech2RIR/\n",
        "!bash download_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msOSPJzwbqgm",
        "outputId": "31ad21ba-edad-43cc-c9c6-07d6a06d55bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Speech2RIR'...\n",
            "remote: Enumerating objects: 237, done.\u001b[K\n",
            "remote: Counting objects: 100% (237/237), done.\u001b[K\n",
            "remote: Compressing objects: 100% (228/228), done.\u001b[K\n",
            "remote: Total 237 (delta 55), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (237/237), 214.23 KiB | 7.39 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/Speech2RIR\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CcF1c9i76-MVPJ-PGoBwvaVtOUBPD57y\n",
            "From (redirected): https://drive.google.com/uc?id=1CcF1c9i76-MVPJ-PGoBwvaVtOUBPD57y&confirm=t&uuid=e7f634d3-4a6c-47f2-8f72-a84f21b8213b\n",
            "To: /content/Speech2RIR/checkpoint-1040000steps.pkl\n",
            "100% 3.38G/3.38G [00:35<00:00, 95.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TrnXjR-vrCrub_RY6kdEBqA0D4hGiIp3\n",
            "From (redirected): https://drive.google.com/uc?id=1TrnXjR-vrCrub_RY6kdEBqA0D4hGiIp3&confirm=t&uuid=088836e6-6780-486b-8808-e53310b45dfe\n",
            "To: /content/Speech2RIR/checkpoint-1900000steps.pkl\n",
            "100% 3.38G/3.38G [00:43<00:00, 78.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CARICO IL MODELLO:\n",
        "\n",
        "- Importa le librerie — carica il codice di PyTorch e della rete (modello) specifico che userai.\n",
        "\n",
        "- Istanzia il modello — crea una nuova rete (modello) usando il codice della classe Generator. Qui devi specificare le stesse dimensioni e parametri con cui la rete è stata addestrata, altrimenti i pesi non ci entreranno bene.\n",
        "\n",
        "- Carica i pesi del modello — apre il checkpoint (file con dati salvati) e legge i pesi nella parte giusta del file (checkpoint['model']['generator']).\n",
        "\n",
        "- Inserisci i pesi nel modello — qui è il punto critico: se le dimensioni del modello e dei pesi non combaciano esce errore.\n",
        "\n",
        "- Metti il modello in modalità valutazione — prepara il modello per fare previsioni (usare il modello, non addestrarlo).\n",
        "\n",
        "- Stampa messaggio di conferma."
      ],
      "metadata": {
        "id": "QbiN2-Iy1AsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I pesi verranno utilizzati nella funzione forward del generator, in particolare nell'encoder e nel decoder. Questi due usano pesi interni es.\n",
        "\n",
        "*conv = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3*)\n",
        "\n",
        "- Prima di caricare pesi, i valori dei filtri (kernel) sono casuali.\n",
        "\n",
        "- Dopo conv.load_state_dict(...) con pesi addestrati, quei valori cambiano e diventano precisi per rilevare certe caratteristiche nell'input.\n",
        "\n",
        "- Quando passi un input x a conv(x), il risultato riflette le trasformazioni con quei pesi."
      ],
      "metadata": {
        "id": "pxorWRtd5aSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Import delle librerie\n",
        "import torch\n",
        "from models.autoencoder.AudioDec import Generator\n",
        "\n",
        "# 2️⃣ Crea il modello (con i parametri di default)\n",
        "# Parametri dal file di configurazione YAML\n",
        "model = Generator(\n",
        "    input_channels=1,\n",
        "    output_channels_rir=1,\n",
        "    encode_channels=16,\n",
        "    decode_channels=16,\n",
        "    code_dim=128,\n",
        "    codebook_num=64,\n",
        "    codebook_size=8192,\n",
        "    bias=True,\n",
        "    combine_enc_ratios=[],\n",
        "    combine_enc_strides=[],\n",
        "    seperate_enc_ratios_rir=[2, 4, 8, 12, 16, 32],\n",
        "    seperate_enc_strides_rir=[2, 2, 3, 5, 5, 5],\n",
        "    rir_dec_ratios=[256, 128, 64, 32, 32, 32, 16],\n",
        "    rir_dec_strides=[5, 5, 2, 2, 2, 1, 1],\n",
        "    mode='causal',\n",
        "    codec='audiodec',\n",
        "    projector='conv1d',\n",
        "    quantier='residual_vq',\n",
        ")\n",
        "\n",
        "# 3️⃣ Carica i pesi pre-addestrati\n",
        "checkpoint_path = \"exp/autoencoder/symAD_vctk_48000_hop300/checkpoint-1900000steps.pkl\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "# Carica nello stato del modello solo la parte 'generator' dentro 'model'\n",
        "model.load_state_dict(checkpoint['model']['generator'])\n",
        "\n",
        "# 4️⃣ Metti il modello in modalità \"valutazione\"\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Modello caricato correttamente!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECJNCrMFlS1j",
        "outputId": "b6de39d6-1c98-4844-b87b-b8b74e845488"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 131MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modello caricato correttamente!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- carico e preparo l'audio\n",
        "- stimo la RIR\n",
        "- salvo"
      ],
      "metadata": {
        "id": "0vy9D0gAKEDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "# 1️⃣ Carica l’audio\n",
        "audio_path = \"esempio.wav\"\n",
        "waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "# 2️⃣ Porta il sample rate a 48000 Hz se serve\n",
        "if sr != 48000:\n",
        "    waveform = torchaudio.functional.resample(waveform, sr, 48000)\n",
        "    sr = 48000\n",
        "\n",
        "# 3️⃣ Converte in mono (il modello accetta 1 canale)\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "# 4️⃣ Aggiungi dimensione batch (B, C, T)\n",
        "waveform = waveform.unsqueeze(0)\n",
        "\n",
        "# 5️⃣ Stima la RIR\n",
        "with torch.no_grad():\n",
        "    estimated_rir = model(waveform)\n",
        "\n",
        "# 6️⃣ Risultato\n",
        "print(\"✅ RIR stimata!\")\n",
        "print(\"Shape:\", estimated_rir.shape)\n"
      ],
      "metadata": {
        "id": "ditPHrOPKPOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}